# train: loss = max(0, 0.5 - P(pos) + P(neg))
# Test accuracy reaches 91.6 after 80 training steps
# python ./src/run_lm_train_copa.py --block_size=50 --model_name_or_path=/home/niuyilin/pre-trained-models/roberta-large --output_dir=./runs/RoBERTa-large-LM-trainONcopa-0.5marginPosNegProb --num_train_epochs=4 --learning_rate=1e-5 --model_type=roberta --do_train --train_data_file=./data/COPA/COPA-resources/datasets/copa-dev.xml --do_eval --eval_data_file=./data/COPA/COPA-resources/datasets/copa-test.xml --per_gpu_train_batch_size=4 --per_gpu_eval_batch_size=4 --gradient_accumulation_steps=2 --mlm --evaluate_during_training --save_steps=10 --logging_steps=10
# eval: without finetuning
# Test accuracy reaches 75.0 without finetuning
python ./src/run_lm_train_copa.py --block_size=50 --model_name_or_path=/home/niuyilin/pre-trained-models/roberta-large --output_dir=/home/niuyilin/pre-trained-models/roberta-large --num_train_epochs=4 --learning_rate=1e-5 --model_type=roberta --train_data_file=./data/COPA/COPA-resources/datasets/copa-dev.xml --do_eval --eval_data_file=./data/COPA/COPA-resources/datasets/copa-test.xml --per_gpu_train_batch_size=4 --per_gpu_eval_batch_size=4 --gradient_accumulation_steps=2 --mlm